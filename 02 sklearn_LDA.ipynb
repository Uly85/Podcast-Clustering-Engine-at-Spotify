{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc92dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re, nltk\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "podcast_stopwords = ['support', 'podcast', 'https', 'http', '', 'follow', 'us', 'on', 'instagram', 'twitter', 'youtube', 'facebook']\n",
    "stopwords += podcast_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd80774",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dat = pd.read_csv('../script_output/episode_transcript_data_w_metadata.csv')\n",
    "raw_dat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fa4021",
   "metadata": {},
   "source": [
    "index\tshow_uri\tshow_name\tshow_description\tpublisher\tlanguage\trss_link\tepisode_uri\tepisode_name\tepisode_description\tduration\tshow_filename_prefix\tepisode_filename_prefix\tUnnamed: 0\tepisode\ttranscript\n",
    "0\t0\tspotify:show:2NYtxEZyYelR6RMKmjfPLB\tKream in your Koffee\tA 20-something blunt female takes on the world...\tKatie Houle\t['en']\thttps://anchor.fm/s/11b84b68/podcast/rss\tspotify:episode:000A9sRBYdVh66csG2qEdj\t1: It’s Christmas Time!\tOn the first ever episode of Kream in your Kof...\t12.700133\tshow_2NYtxEZyYelR6RMKmjfPLB\t000A9sRBYdVh66csG2qEdj\t34866\t000A9sRBYdVh66csG2qEdj\tHello. Hello. Hello everyone. This is Katie an...\n",
    "1\t1\tspotify:show:15iWCbU7QoO23EndPEO6aN\tMorning Cup Of Murder\tEver wonder what murder took place on today in...\tMorning Cup Of Murder\t['en']\thttps://anchor.fm/s/b07181c/podcast/rss\tspotify:episode:000HP8n3hNIfglT2wSI2cA\tThe Goleta Postal Facility shootings- January ...\tSee something, say something. It’s a mantra ma...\t6.019383\tshow_15iWCbU7QoO23EndPEO6aN\t000HP8n3hNIfglT2wSI2cA\t14162\t000HP8n3hNIfglT2wSI2cA\tThere were two more murders 15 miles away arri...\n",
    "2\t2\tspotify:show:6vZRgUFTYwbAA79UNCADr4\tInside The 18 : A Podcast for Goalkeepers by G...\tInside the 18 is your source for all things Go...\tInside the 18 GK Media\t['en']\thttps://anchor.fm/s/81a072c/podcast/rss\tspotify:episode:001UfOruzkA3Bn1SPjcdfa\tEp.36 - Incorporating a Singular Goalkeeping C...\tToday’s episode is a sit down Michael and Omar...\t43.616333\tshow_6vZRgUFTYwbAA79UNCADr4\t001UfOruzkA3Bn1SPjcdfa\t93168\t001UfOruzkA3Bn1SPjcdfa\tWelcome to inside the 18. Today's episode is t...\n",
    "3\t3\tspotify:show:5BvKEjaMSuvUsGROGi2S7s\tArrowhead Live!\tYour favorite podcast for everything @Chiefs! ...\tArrowhead Live!\t['en-US']\thttps://anchor.fm/s/917dba4/podcast/rss\tspotify:episode:001i89SvIQgDuuyC53hfBm\tEpisode 1: Arrowhead Live! Debut\tJoin us as we take a look at all current Chief...\t58.189200\tshow_5BvKEjaMSuvUsGROGi2S7s\t001i89SvIQgDuuyC53hfBm\t69703\t001i89SvIQgDuuyC53hfBm\tHey cheese fans before we get started. I wante...\n",
    "4\t4\tspotify:show:7w3h3umpH74veEJcbE6xf4\tFBoL\tThe comedy podcast about toxic characters, wri...\tEmily Edwards\t['en']\thttps://www.fuckboisoflit.com/episodes?format=rss\tspotify:episode:0025RWNwe2lnp6HcnfzwzG\tThe Lion, The Witch, And The Wardrobe - Ashley...\tThe modern morality tail of how to stay good f...\t51.782050\tshow_7w3h3umpH74veEJcbE6xf4\t0025RWNwe2lnp6HcnfzwzG\t104381\t0025RWNwe2lnp6HcnfzwzG\tSorry to interrupt the show, but I do have to ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d8e40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dat = raw_dat.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ad5ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dat['show_description_cleaned'] = raw_dat['show_description'].str.lower()\n",
    "# raw_dat['show_description_cleaned'] = raw_dat['show_description'].str.lower()\n",
    "# raw_dat['show_episode_description_concat'] = raw_dat['episode_description_cleaned'] + raw_dat['show_description_cleaned']\n",
    "noise = ['@', 'www', 'http', 'discord.gg', 'social media:', 'on social media', 'new episodes', 'latest episodes', 'episode', 'anchor']\n",
    "for n in noise: \n",
    "    raw_dat['show_description_cleaned'] = raw_dat['show_description_cleaned'].str.replace(f'\\S*{n}\\S*\\s?', '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c00da90",
   "metadata": {},
   "source": [
    "<>:6: DeprecationWarning: invalid escape sequence \\S\n",
    "<>:6: DeprecationWarning: invalid escape sequence \\S\n",
    "<>:6: DeprecationWarning: invalid escape sequence \\S\n",
    "<>:6: DeprecationWarning: invalid escape sequence \\S\n",
    "/var/folders/7r/zn5kpnx53gg8tgmvvwp861s00000gn/T/ipykernel_1906/2394576055.py:6: DeprecationWarning: invalid escape sequence \\S\n",
    "  raw_dat['show_description_cleaned'] = raw_dat['show_description_cleaned'].str.replace(f'\\S*{n}\\S*\\s?', '', regex=True)\n",
    "/var/folders/7r/zn5kpnx53gg8tgmvvwp861s00000gn/T/ipykernel_1906/2394576055.py:6: DeprecationWarning: invalid escape sequence \\S\n",
    "  raw_dat['show_description_cleaned'] = raw_dat['show_description_cleaned'].str.replace(f'\\S*{n}\\S*\\s?', '', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27044d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_titles(title):\n",
    "    tokens = nltk.word_tokenize(title)\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    filtered_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        token = token.lower()\n",
    "        token = token.replace(\"'s\", \" \").replace(\"n’t\", \" not\").replace(\"’ve\", \" have\")\n",
    "        token = re.sub(r'[^a-zA-Z]', '', token)\n",
    "        token = re.sub(r'\\b\\w*bitly\\w*\\b', '', token)\n",
    "        token = re.sub(r'\\b\\w*com\\b', '', token)\n",
    "\n",
    "        if token not in stopwords:\n",
    "            filtered_tokens.append(token.lower())       \n",
    "    \n",
    "    lemmas = [lmtzr.lemmatize(t,'v') for t in filtered_tokens]\n",
    "\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4033d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dat['show_description_tokens'] = raw_dat.show_description_cleaned.apply(tokenize_titles)\n",
    "raw_dat['show_description_preprocessed'] = raw_dat.show_description_tokens.str.join(' ')\n",
    "# raw_dat.to_csv('../script_output/02_final_dat.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f8d66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                # tokenizer=tokenize_titles,\n",
    "                                max_features=500,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.8, \n",
    "                                min_df = 20,\n",
    "                                ngram_range=(2,4))\n",
    "dtm_tf = tf_vectorizer.fit_transform(raw_dat.show_description_preprocessed)\n",
    "print(dtm_tf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da19bb5",
   "metadata": {},
   "source": [
    "(105153, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9996af",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_tf = LatentDirichletAllocation(n_components=5, \n",
    "                                   max_iter=50,\n",
    "                                   learning_method='online', \n",
    "                                   random_state = 0, \n",
    "                                   n_jobs=-1)\n",
    "lda_tf.fit(dtm_tf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1176cb",
   "metadata": {},
   "source": [
    "LatentDirichletAllocation(learning_method='online', max_iter=50, n_components=5,\n",
    "                          n_jobs=-1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfcd015",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 100\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "topics = dict()\n",
    "for topic_idx, topic in enumerate(lda_tf.components_):\n",
    "    topics[topic_idx] = [tf_feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "    \n",
    "topics_df = pd.DataFrame(data=topics)\n",
    "topics_df.columns = [f'topic_{t}' for t in list(topics.keys())]\n",
    "# topics_df.to_csv('../script_output/02_LDA_topics.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c26d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(lda_tf, dtm_tf, tf_vectorizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
